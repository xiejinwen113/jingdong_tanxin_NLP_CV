10:05:42	  46149090 : 1
10:20:00	  96459666 : 麻木 哈哈哈
10:20:10	  52007964 : 受内伤了
10:21:27	  52007964 : 什么时候能把人脑神经的激活函数解析出来
10:21:48	  kk60 : 能解释出来就无敌了
10:22:13	  46149090 : 特斯拉老板不在研究吗
10:22:36	  52007964 : 能解出来还要GPU来算
10:23:16	  晶品王 : 那得要量子计算来算了吧
10:23:45	  52007964 : 人脑经常给一两个训练数据，就学会了
10:23:54	  52007964 : 迁移能力超强
10:28:19	  滑车 : relu 的输出始终大于等于0，是否也有 偏置现象？
10:30:24	  kk60 : 偏置现象具体的坏处是啥啊
10:31:07	  52007964 : Leak一下
10:32:20	  kk60 : 偏置现象具体的坏处是啥啊
10:32:23	  kk60 : relu 的输出始终大于等于0，是否也有 偏置现象？
10:32:39	  62305729 : 不知道什么时候该用什么激活函数
10:32:39	  46149090 : lstm中默认的tanh可以换成relu吗
10:32:52	  Will : 循环神经网络一般用哪个激活函数呢？
10:33:08	  96459666 : tanh比较多
10:33:10	  63942550 : 循环一般用tanh
10:33:27	  滑车 : 神经元中的偏置项 b ，是否可以修正偏置误差？
10:33:30	  limingkailmk : 为什么啊
10:33:35	  kk60 : 为啥效果不太好
10:34:03	  46149090 : lstm中默认的tanh可以换成relu吗，代码封装的时候relu也是可选项
10:34:07	  52007964 : 所以可以理解，神经网络最初是为了解决图像识别的问题，后来发展出了深度学习，再后来发现深度学习可以用在数据分析Nlp各种AI方向上
10:35:02	  Jiehan Zhu : 多层感知机 是 fully connect 么？
10:35:23	  lyj : 神经元中的偏置项 b ，是否可以修正偏置误差？
10:35:33	  kk60 : 还是不太理解偏置现象的坏处，可以再说一下吗
10:35:36	  shisihuan24 : 偏置的坏处是啥?
10:36:52	  94949957 : BN
10:36:56	  胡云聪 : 如果你x输出全正，那么对应到W的一行，梯度更新时要么全正要么全负，做不到个别正个别负
10:37:58	  Jiehan Zhu : 但我们放进下一层的时候的时候是不是就能靠那一层的b调整分布区间了？
10:38:02	  kk60 : sigmoid不也是全部0~1
10:38:06	  kk60 : 咋就不稳定了
10:38:49	  kk60 : 噢 我懂了
10:38:53	  data001 : 偏置现象是和数据分布相差很大，可以这样理解么
10:38:56	  滑车 : 和正负没关系吧，只要 -0.5，就有正有负，关键是分布
10:39:18	  kk60 : 嗯，我能理解为什么relu比sigmoid好了
10:39:24	  于对对 : 只要加了激活函数就算黑箱模型了hh，没法解释了
10:40:59	  滑车 : 所以偏置现象的关键在于存在固定的阈值，而不是大于 0 ？
10:41:06	  滑车 : 值域
10:41:36	  GeekDream : 稳定不就好了吗 为什么w一定要小呀？
10:41:36	  滑车 : sigmoid 和 tanh 容易偏置，relu 没有?
10:41:55	  王友全 : 可以理解为把鱼都赶到一个特定区域来捕捞吗
10:42:05	  JudyTsai : 所以通常是全部訓練數據集一起做normalization還是分batch做呢? batch normalization通常是怎麼做呢?
10:43:04	  liuf : sigmoid容易梯度消失
10:43:06	  kk : 同问，数据归一化和BN什么关系？
10:43:20	  胡云聪 : 数据归一化在输入层，BN在中间层
10:43:32	  kk60 : 是不是可以继续了 hhh
10:43:34	  lyj : 同意，专题讲讲
10:43:35	  cath_kl : 老师我们继续往下讲吧
10:43:36	  kk : 继续吧
10:43:39	  GeekDream : 继续吧老师 才讲了1/5
10:43:43	  JudyTsai : 謝謝老師~很需要NORMALIZATION的課程
10:43:44	  shisihuan24 : 继续
10:43:51	  20200203077 : @52007964 深度学习是针对传统机器学习来说的，深度学习的代表是神经网络，神经网络最初是为了解决感知机过于简单无法模拟复杂问题。至于cv nlp 等都是实际应用场景。
10:50:13	  cath_kl : 1
10:50:14	  kk60 : 没
10:50:18	  46149090 : 没
10:50:20	  Lee : 没
10:54:08	  kk60 : MSE可以用在分类问题吗
10:54:23	  kk60 : 还是说可以用 但是效果不好？
10:54:30	  62305729 : 可以，效果不好
10:54:34	  Lee : 应该是效果不好吧
10:54:41	  kk60 : OK
10:55:01	  JudyTsai : 那多分類問題也是用交叉商嗎?
10:55:25	  kk60 : 就是多分类采用交叉熵
10:55:31	  kk60 : *才
10:56:28	  hbfuyue : 什么是正则项？
10:56:38	  yukong : 结构风险是variance？
10:56:55	  kk60 : 正则就是防止overfitting的一个technique
10:57:14	  有朝一日 : 正则项可以理解为控制模型复杂度的
10:57:15	  Lee : 感觉可以理解为交叉熵对误差的估计考虑了数据分布的情况，MSE的假设可以认为是均匀分布
10:57:36	  46149090 : 正则项是用来约束参数波动的
10:57:45	  GeekDream : 学过
10:57:57	  于对对 : 学过
10:58:08	  Lee : 正则项是约束参数，主要是防止过拟合的
10:58:20	  hbfuyue : 谢谢
10:59:02	  44040788 : 卡住了么
10:59:29	  kk60 : L1是不是相比较L2会产生更多值为0的wieght？有点忘了
11:01:46	  zhijian_official : L1 -> Weight更稀疏
11:01:54	  zhijian_official : L2 -> Weight更小
11:02:33	  kk60 : 谢谢
11:07:06	  46149090 : 代价函数尽量是凸函数才能这么求解吧
11:08:00	  hyalin : 都可以，差別是全局最優或局部最優而已
11:11:19	  Lee : 因为算起来方便？
11:24:10	  滑车 : 关于偏置现象的还是有疑问。sigmoid 因为取值大于 0 而有偏置，tanh 在 [-1,1] 之间所以不会出现偏置。那么 定义一个新函数 f = sigmoid - 0.5, 新函数的取值在 [-0.5,0.5] 之间 是否就可以避免 偏置现象？
11:24:47	  kk60 : 可以 但是0.5这个数字数学意义不大
11:25:10	  kk60 : sigmoid之所以用的多有很大程度因为他可以用来表示概率
11:25:10	  GeekDream : 为什么要避免偏置呢
11:25:29	  zhijian_official : sigmoid 主要不用还是因为梯度消失吧
11:25:54	  kk60 : 也不是不用 sigmoid一般只用在最后一层
11:26:01	  滑车 : 根本上是没有理解，到底啥是偏置现象？
11:26:03	  GeekDream : 别的一样有梯度消失啊
11:26:14	  kk60 : 想什么逻辑回归都只能用sigmoid做二分类
11:26:26	  zhijian_official : 中间层很少用 sigmoid
11:26:36	  滑车 : 老师能给出偏置现象的英文术语吗？中文检索不到相关信息
11:26:43	  zhijian_official : 以前用过，难收敛
11:26:56	  树第 : 讲完了了
11:27:06	  kk60 : google 搜索 why relu is better than sigmoid
11:27:17	  kk60 : 有一堆文献可以参考下
11:27:23	  billyzhang24kobe : Bias term?
11:27:35	  kk60 : 不是bias term....
11:27:43	  kk60 : 那是完全另外个东西
11:27:55	  GeekDream : 你这是偏置项 人家说偏置现象。。。
11:28:28	  billyzhang24kobe : 哦哦搞错了 sorry
11:30:27	  滑车 : relu 和 sigmoid  都有偏置现象
11:30:38	  滑车 : 我搜 why relu is better than sigmoid有啥用？
11:31:07	  胡云聪 : 你想说的是why zero centered activation function
11:31:12	  胡云聪 : 这样的？
11:31:34	  滑车 : 我不明白老师说的 偏置现象 到底是啥？
11:31:55	  GeekDream : 确实搜不到
11:31:57	  胡云聪 : Why is the zero-centered activation function preferred in neural networks?大概这么搜试试
11:32:00	  GeekDream : 可能老师自创的。。
11:32:17	  胡云聪 : 这个cs231n也提到了，不是自创的
11:32:23	  滑车 : 嗯嗯，谢谢 胡云聪~
11:32:36	  GeekDream : cs231n是啥呀大佬
11:32:47	  胡云聪 : 斯坦福的机器视觉课
11:33:22	  王友全 : 我咋个听不到了呢
11:33:26	  GeekDream : 好叻谢谢
11:33:37	  22593442 : 我听见有人说话啦。。
11:33:43	  王友全 : 有了
11:34:04	  kk60 : why tanh is better than sigmoid有啥用？
11:34:09	  kk60 : 我打错了
11:34:11	  kk60 : tanh
11:34:15	  胡云聪 : 他们给出的解释是，如果输出全正，那么在梯度在回传时，对于系数矩阵的一行更新梯度时，要么就全正要么全负，那梯度更新相当于少了很多方向
11:34:31	  王友全 : 我只看到休息那个ppt
11:34:42	  滑车 : 嗯，谢谢老师，谢谢胡云聪~~
11:34:53	  Leonzz : 是的，我也看到这个解释
11:34:57	  GeekDream : 谢谢大佬聪
11:35:21	  Leonzz : https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation#:~:text=Sigmoid%20outputs%20are%20not%20zero,that%20is%20not%20zero%2Dcentered.
11:40:52	  kk60 : 这才是卷积。。。作业1那个卷积也是醉了
11:42:45	  赵诚宇 : 这个乘法是对应位置相乘吗
11:42:54	  kk60 : 是
11:43:10	  kk60 : 对应相乘求和得一个值
11:43:15	  GeekDream : 往下走多少也是根据步长多少吗
11:45:22	  GeekDream : 谢谢老师
11:46:41	  JudyTsai : 卷積的運算一定是相乘相加嗎?
11:48:00	  96459666 : 谁没静音啊。麻烦静音
11:49:00	  sc_lj : 谁把声音打开了
11:49:18	  sc_lj : 麻烦静下音
11:49:25	  kk60 : 没人开好像
11:49:28	  kk60 : 老师那边的
11:49:42	  Francesca : 94949957这个人
11:49:58	  kk60 : 他不是主持人么。。。
11:50:09	  ZY : 那是老师吧
11:50:44	  赵诚宇 : 这个池化会不会遇上不能整除的情况
11:52:15	  kk60 : convolution-pool pair一般CNN里面需要多少？算是hyper-parameter么
11:52:48	  赵诚宇 : 我是说矩阵的边长
11:52:59	  赵诚宇 : 比如一开始是17*17
11:53:11	  szb : 可以做Padding
11:53:19	  JudyTsai : 那要padding?
11:53:28	  赵诚宇 : 好的谢谢
11:54:11	  hbfuyue : 卷积核中的值，是人为给定的？还是学习出来的？
11:54:21	  Lee : 学的
11:54:32	  kk60 : 这就是CNN的weight
11:55:08	  hbfuyue : 为什么会学习出不同卷积核？每次学习的结果都不一样吗？
11:55:29	  Lee : 会的，因为初始化不一样
11:55:37	  kk60 : 一个卷积核是一个特征提取器，多个卷积核代表多个特征
11:56:07	  hbfuyue : 本例中有两个卷积核
11:56:08	  shisihuan24 : 为什么要多个卷积核?
11:56:21	  GeekDream : 多个卷积核提取多个特征啊
11:56:25	  kk60 : 卷积核就是特征提取器啊
11:56:26	  hbfuyue : 老师讲的例子中有两个卷积核
11:56:31	  lyj : 不同的卷积核可以获得不同的局部特征
11:56:41	  kk60 : 比如一个卷积核提取斜边，另外一个提取圆形
11:56:48	  王友全 : 多个核就多个视角
11:57:25	  hbfuyue : 如何控制某个卷积核要提取什么特征？
11:57:36	  95960842 : 卷积核里面值 是学习来的吗？
11:57:59	  Lee : 控制那就不是学习出来的了，是手工特征了
11:58:04	  五九 : 这样的卷积不可逆吧，没法解卷积？
11:58:12	  cath_kl : 卷积是相当于weight吗
11:58:19	  hbfuyue : 如何针对不同的特征，学习不同的卷积核？
11:58:32	  szb : 自动学的
11:58:39	  limingkailmk : 卷积核参数如何更新啊
11:59:00	  46149090 : 平摊是放在CNN最后吗，可以放在中间层吗
11:59:20	  kk60 : 平摊就是分类，平摊之前都可以理解成为特征提取
11:59:26	  lyj : 根据任务目标，学出利于解决任务的参数
11:59:42	  孑然侧看 : 卷积也可以理解为广义全连接，将图片大小和卷积核大小设置一致；
12:02:12	  hbfuyue : 如何针对不同的特征，学习不同的卷积核？答案是：其实，就是多次随机初始化卷积核参数，然后，通过学习，得到不同的卷积核。对吧？
12:02:43	  hbfuyue : 需要多次随机初始化
12:03:05	  kk60 : 为啥要多次初始化
12:03:28	  hbfuyue : 一次随机初始化，只能得到一个卷积核？
12:03:37	  hbfuyue : 只能得到一个特征
12:03:57	  kk60 : 一次性全部初始化，全部一起学习，共同更新
12:04:12	  hbfuyue : 一次性随机初始化多个卷积核
12:04:16	  kk60 : 一个卷积核
12:04:17	  Lee : 一次初始化所有的卷积核，然后随着更新每次各个卷积核会往对应特征上面靠
12:04:22	  kk60 : 你理解成一个neuron
12:04:38	  kk60 : 就是神经网络里的一个节点，不知道这样你能不能理解
12:05:51	  hbfuyue : 理解，谢谢kk60
12:09:46	  57325467 : 加了4个padding
12:10:49	  Lee : 5*5*6
12:10:51	  有朝一日 : 150
12:10:59	  孑然侧看 : 少了bias
12:11:03	  胡云聪 : 5*5+1
12:13:05	  Matt_SH : 池化为什么有参数啊
12:13:49	  GeekDream : 池化w b 是什么啊
12:14:18	  57325467 : 超参数呢
12:14:43	  Matt_SH : 超参数就比如学习率 batch_size这种
12:15:10	  ls : C3层是每个卷积核跟每个feature map都算出一个新的feature map吗？
12:15:22	  lyj : 对于三通道的图片，卷积核的大小是三维的吗？
12:16:41	  lyj : 了解了，谢谢老师
12:16:58	  胡云聪 : 有些网络没用用简单的池化，而是用了卷积核大stride做池化，所以才写了参数吧
12:17:25	  胡云聪 : ResNet就是用的卷积核
12:29:36	  Matt_SH : 是对哪个维度进行池化？
12:30:00	  46149090 : filter维度吗
12:30:12	  46149090 : 还是句子长度
12:30:40	  83591009 : 卷积核一般怎样初始化？随机吗？
12:31:03	  46149090 : 两边都可以做池化是吧
12:31:08	  kk60 : 为什么7 X 5卷积后变成4X5了。。
12:31:18	  72352735 : 句子长度吧
12:31:22	  83591009 : 池化是针对卷积后的结果
12:31:36	  kk60 : 对
12:31:42	  胡云聪 : 因为没padding
12:31:45	  ls : 卷积完了以后，就没有词向量的维度了吗
12:31:46	  胡云聪 : 吧
12:32:04	  46149090 : 卷积完成没有词向量维度了
12:32:07	  liuf : （7+1-4）/1
12:32:11	  kk60 : 噢噢，卷积核可以不是正方形的是吧
12:32:11	  46149090 : 但是有filer维度
12:32:28	  ls : 哦
12:33:02	  kk60 : 第一个的卷积核是2X5？
12:33:14	  46149090 : 是的
12:33:17	  kk60 : 那卷完不是成 6X 1了
12:33:21	  kk60 : 为啥是4X5
12:33:22	  46149090 : 是的
12:33:28	  46149090 : 4个卷积核
12:33:57	  kk60 : 不是吧。。。为啥5就不变了
12:34:11	  Matt_SH : 这里是一维卷积，保证词向量维度
12:34:18	  kk60 : 噢
12:34:20	  kk60 : 这样
12:45:07	  韩晓东 : 谢谢老师 辛苦了
