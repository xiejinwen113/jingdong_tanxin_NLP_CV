20:30:19	  46149090 : 有声音？？
20:30:25	  kexin : Mei sheng yin?
20:30:28	  99873296 : 没有
20:30:28	  kf42 : 老师 测下声音
20:30:38	  83591009 : 没声音
20:30:41	  lyj : 1
20:30:42	  kexin : 1
20:30:42	  68241514 : 1
20:30:43	  韩晓东 : 1
20:30:44	  83591009 : 可以
20:30:46	  46149090 : 1
20:30:47	  hpchen : 1
20:44:38	  sc_lj : 如何unpooled的呢？
20:44:42	  liuf : 老师 这个反卷积是不需要学习权值的是吧，那跟upsample 在效果上的区别
20:44:45	  Vince : 反卷积kernal的权重是什么？
20:45:04	  Vince : 是直接使用卷积的权重吗？
20:45:29	  Eason060 : 得到unpooled map有什么用呀
20:46:11	  sc_lj : 从全链接层开始反的吗？
20:46:30	  billyzhang24kobe : 请问switches的作用是什么呢
20:46:33	  Eason060 : 噢噢
20:46:51	  SimonChen : 记录池化位置
20:47:26	  SimonChen : 大家上paper课的话可以先快速浏览一下abstract和introduction，这样听起来不会懵逼~
20:52:06	  有朝一日 : 没有
20:52:09	  99873296 : 没有
20:52:10	  晶品王 : 我有个问题，计算走向不是从右往左走的吗？正向走的时候能记录max后的，为什么不再往前记录一下max前的？
20:52:15	  赢华控股 : 没有 
20:52:18	  孟广浩 : 没有
20:52:30	  shisihuan24 : 继续吧
20:52:31	  limingkailmk : 不进行操作的其他层如何屏蔽的
20:54:07	  可立 : 如果输出是0呢
20:54:25	  BUG_MAN : 这里不会有负数吧?
20:54:26	  Vince : 现在用mish swish激活函数怎么处理呢？
20:54:45	  lyj : 正向过程为啥要用ReLU来保证是正值呀？
20:54:55	  sc_lj : 对relu的反操作采用relu？
20:55:29	  shisihuan24 : 不要问为什么 人家就只这样干有效果
20:55:38	  hbfuyue : 负数经过relu就会变成0，反卷积应该变回负数呀？我觉得
20:55:49	  Vince : 对于其他的激活函数怎么办？
20:55:50	  99873296 : 不应该用relu的反函数吗
20:56:01	  ttsama : 不太明白为什么这么做，没数学公式好难受
20:56:20	  BUG_MAN : 为什么装置就可以decon？
20:56:22	  可立 : 诶？为啥是转置，不应该是取逆吗？
20:56:30	  Loop : 不用还原负数吗
20:56:33	  sc_lj : 反卷机的参数是啥？需要学习吗？
20:56:46	  晶品王 : 我大概明白了，如果取max前的，就没意义了吧，本来就是要看max pool以后的效果怎么样
20:57:09	  ls : 它的思路应该不是完全返回去吧，如果能完全返回去，不就成原图了吗
20:57:25	  滑车 :  transpose 卷积的理由是什么呀？
20:57:32	  lyj : 正向过程为啥要用ReLU来保证是正值？
20:57:36	  王友全 : 我理解成了要返回到输入层
20:57:37	  有朝一日 : 就是屏蔽了一部分
20:57:43	  limingkailmk : 不进行操作的其他层如何屏蔽的
20:57:45	  hbfuyue : 不是完全复原原图呀？
20:57:47	  Jiehan Zhu : 这个和直接在pixel上计算grad of each layer有什么区别？
20:57:50	  晶品王 : @ls，对，是这个意思，我明白了
20:58:06	  滑车 : 为什么转置呢？
20:58:08	  shisihuan24 : 不用训练?
20:58:20	  zhijian_official : 文章还是想知道什么输入会激活输出，那么在 rule 中被抑制的输入，那对于输出来说本身就不重要，所以不复原回去也能理解吧
20:58:33	  zhijian_official : 个人感觉
20:58:47	  20200203077 : 若是平均池化 怎样逆操作？
20:58:50	  Jiehan Zhu : 这个和直接在pixel上track grad of each layer有什么区别？
20:58:50	  Loop : 这样反卷积为啥就可以可视化了呢。。
20:58:51	  滑车 : 为啥转置呀？
20:58:56	  lyj : transposed还原的过程能不能举个例子
20:59:02	  ww5365 : +1
20:59:04	  可立 : 为什么是转置而不是取逆
20:59:11	  Jiehan Zhu : 我们算然不训练，但我们可以计算grad
20:59:18	  shisihuan24 : 直接用Alexnet训练的参数??????
20:59:26	  BeyondRiver : 要保证是同样的卷积核吧
20:59:35	  ChenS : 在conv 和 deconv时，用一样的relu吗
20:59:59	  Matt_SH : 卷积不是矩阵计算吧？ 不是对应元素点乘吗，所以不取逆吧。
21:00:05	  滑车 : 然后发现随便怎么算，最后都能给出各种看似合理的解释。。。。
21:00:06	  sc_lj : 卷积核不一定都能可逆吧
21:00:13	  Loop : 为什么deconvnet就表示学到的特征呢
21:00:27	  可立 : 伪逆
21:00:28	  Jiehan Zhu : what is the difference between this and salience map?
21:00:40	  滑车 : 我个人强烈怀疑可视化的合理性
21:01:01	  滑车 : 出来个东西，然后各种套~各种合理
21:01:05	  20200203077 : 平均池化取逆是把均值填到每个空位么
21:01:07	  王友全 : 卷积也不是矩阵乘法，所以不用取逆
21:01:09	  ls : 应该是有原因的。。
21:01:10	  BeyondRiver : 不是完全可视化 就是一种解释方法
21:01:19	  BeyondRiver : 为了更好的理解黑箱
21:01:30	  滑车 : 盲人摸象
21:01:33	  xiaoliliu : 我看了这个推导，挺合理的
21:01:35	  xiaoliliu : 如何理解深度学习中的deconvolution networks？ - 张萌的回答 - 知乎https://www.zhihu.com/question/43609045/answer/120266511
21:02:11	  ChenS : 这种visualization有用在在文本数据上过吗
21:02:31	  BeyondRiver : 文本怎么可视化...
21:02:34	  ZY : 工业项目上 有多少人 用这类的帮助调参哇
21:02:52	  Jiehan Zhu : what is the difference between this and salience map?
21:02:52	  limingkailmk : python有函数可以可视化
21:03:03	  limingkailmk : 像素可视化
21:03:45	  滑车 : 文本上，Transformer 的 Attention 可以可视化，但感觉也是很玄学
21:03:50	  Lee : 合理应该是合理的，就是丢掉了很多没有对最终层起作用的信息，最后可视化的结果是有效的，那就是说就是最后学习到了有效特征（忽略掉丢弃的信息）我是这么理解的
21:03:54	  ttsama : 所以反卷积，就是为了看看哪些特征影响了黑盒的结果了
21:04:06	  SimonChen : 用max pooling肯定是有丢失信息的
21:04:29	  liuf : 对 这种方式的反卷积只是为了看学到的东西
21:04:56	  hbfuyue : 看不出来重构效果好呀
21:07:42	  晶品王 : 咋不用同一份图展示每层。
21:08:01	  王友全 : 刚才提到过反卷积网络是要执行到输入层吗
21:08:07	  滑车 : 那样合理性就不明显了。。。
21:08:29	  BeyondRiver : 你用纯黄色图展示第四层没有意义啊
21:08:50	  晶品王 : 比如说layer4的狗，5层都用狗看啊
21:09:04	  hbfuyue : 放大才能看清楚
21:09:27	  ZY : 然后可以选择性的drop layer 呀?
21:09:29	  kexin : 为什么每一层要选不一样的图呀？
21:10:00	  滑车 : 说起来，直接输出正向的结果不是更好吗，为啥要重构？每一层的结果都是有的呀~
21:10:21	  hbfuyue : 不同层选用的图片不一样
21:10:32	  kexin : 对呀，直接输出每次的结果不行吗？
21:10:33	  晶品王 : @滑车，这是我一开是没理解的，用原来的看不就没意义了嘛
21:10:39	  ttsama : 为啥不同层选不一样的图片
21:11:08	  BeyondRiver : 别纠结为什么选不一样图片了吧 就是为了展示效果好
21:11:28	  滑车 : 明明可以直接看到每层的数据，为啥要“还原”？
21:11:32	  ls : 每一层的正向结果不是图片，没法看吧。
21:11:37	  Eason060 : ai po k.....
21:11:37	  有朝一日 : 直接输出就不知道具体学的是哪些
21:11:39	  ls : 是feature map
21:12:02	  滑车 : 还原的也不是图片~
21:12:04	  有朝一日 : 只知道学出来是啥样
21:12:07	  xiaoliliu : 如果一个图片上有两个物体，是要切分开来吗？
21:12:08	  滑车 : 是显示出来的
21:12:23	  ttsama : 每一层看到的都是特征，现在是用特征还原成图片吧
21:12:28	  zhijian_official : 文章是选择让该层响应最大 top-9 图片
21:12:29	  xiaoliliu : 确实是，feature mapping
21:12:35	  liuf : 反卷积后尺寸大点 
21:12:58	  zhijian_official : 直接输出来看不出什么，都很小，接近于0
21:13:27	  滑车 : 越想越觉得不合理，直接把每层的 feature map 可视化出来不好么？为啥要还原
21:13:35	  Lee : @滑车 我理解是这样，pooling操作是不可逆的，这里反向获取输入的话相当于是剔除掉没有对最终结果起作用的信息，看最终起作用的信息是否是有效的
21:14:01	  BeyondRiver : ^
21:14:07	  滑车 : 嗯，有道理，还原其实是过滤掉一些没用的信息
21:14:12	  xiaoliliu : 对于不同通道的也是需要一起展示看吗？
21:14:47	  shisihuan24 : 继续吧
21:14:48	  有朝一日 : pooling取最大，剔除了没用的信息，现在逆回去就是想看看到底最大值对应的是哪些特征算出来的
21:14:58	  Jiehan Zhu : 老师能放大用鼠标highlight sudden jump么？
21:15:04	  Jiehan Zhu : 看不出来
21:17:00	  BeyondRiver : 这结果非常intuitive
21:17:01	  1535508217 : feature invariance(不变性)怎么理解？
21:17:05	  96801284 : 纵坐标是什么？
21:17:32	  zhijian_official : 这个不变性感觉不是很明显
21:17:49	  mdd : 老师，对分类任务平移不变吧，对别的任务可不一定哦
21:18:08	  BeyondRiver : 一开始就说了是alexnet
21:23:07	  shisihuan24 : 什么是中频 频率等级怎么划分?
21:23:24	  666 : 同问
21:23:32	  1535508217 : 同问
21:23:50	  666 : 什么高频、低频，中频
21:24:04	  666 : 什么是高频、低频，中频？
21:24:09	  王友全 : 图像dct变换
21:24:43	  liuf : 高频就是细节变化
21:25:42	  liuf : 反卷积的两种实现方式 本文的方式 和upsample 在效果上的区别有比较么？
21:27:27	  liuf : 但是不用本文的方式吧
21:28:11	  BeyondRiver : 有去模糊的论文推荐么
21:28:16	  BeyondRiver : 最近好像有成果
21:29:16	  1535508217 : 老师在我们微信群里面吗？如果我们看完文章有疑问，可以在微信群里面问，然后老师来答疑吗？
21:29:48	  滑车 : 谢谢老师~
21:29:55	  Lee : 谢谢老师
21:29:56	  韩晓东 : thx
21:29:57	  胡云聪 : 谢谢老师
