19:07:00	  BeyondRiver : 交互具体是？
19:07:15	  shisihuan24 : 怎么实现啊?
19:07:21	  hkc000 : 拼接
19:20:08	  44040788 : 是我总掉线么
19:20:18	  BeyondRiver : 是你
19:23:36	  晶品王 : 这3种有好坏之分吗？
19:24:57	  Eason060 : 第二个可以理解成就是ensemble？
19:25:17	  BeyondRiver : 第二种decision fusion的过程是用模型来学么
19:25:36	  BeyondRiver : 就是学习权重的过程
19:30:26	  96459666 : 万金油bert
19:32:27	  liuf : 皆可embedding
19:38:09	  Leonzz : Bi-GRU 每步output什么呀
19:38:15	  Leonzz : sentiment吗
19:38:38	  SimonChen : dense层是怎么实现的？
19:40:37	  hkc000 : 这个是学习2个任务的吗
19:41:05	  Leonzz : sentiment和emotion什么区别
19:41:09	  BeyondRiver : 正负
19:41:12	  32245254 : attention是干啥呢
19:41:14	  BeyondRiver : emotion很多种
19:41:44	  lyj : 多任务学习能稍微讲一下吗
19:41:53	  ChenS : T, V, A 代表了啥
19:41:59	  35494826 : 输入是什么
19:42:14	  lyj : T-text, V-video, A-audio
19:42:55	  96459666 : 这是一个假新闻
19:42:59	  ChenS : 谢谢
19:43:04	  hkc000 : 项目会涉及多模态的工作吗
19:43:20	  BeyondRiver : 这是我上个hackathon做的项目...
19:43:40	  BeyondRiver : 项目应该会吧 图书分类就可以多模态
19:44:59	  hkc000 : 时域/频域吗
19:45:32	  BeyondRiver : 这个input只是图片？
19:45:35	  96459666 : 像素这块  文字和图片信息分别是GRU和CNN提取的吗
19:45:48	  32245254 : 像素怎么变成频率
19:45:48	  Eason060 : 这个input只有1个类别啊
19:46:03	  BeyondRiver : 没有text body吗
19:46:05	  BeyondRiver : 哦哦
19:46:17	  hbfuyue : 什么是假新闻？
19:46:19	  35494826 : 那为什么有多个gru呢
19:46:34	  Eason060 : 假新闻就是假的新闻
19:46:38	  96459666 : 像素这块  文字和图片信息分别是GRU和CNN提取的吗
19:46:48	  hbfuyue : 假新闻是图片和文字不相符的新闻吗？
19:47:16	  Eason060 : 明天你能赚一个E，就是假新闻
19:49:11	  SimonChen : 这两个部分是同时进行的？
19:49:16	  SimonChen : 紫色和绿色的部分
19:49:28	  滑车 : GAN
19:49:32	  BeyondRiver : event discriminator和分类器的区别是？
19:49:48	  Eason060 : 拼接是怎么个拼接法？直接维度叠加吗
19:50:02	  hkc000 : 2个检测器输出是什么
19:50:04	  BeyondRiver : 拼接是concatenation
19:50:28	  BeyondRiver : 不 我的意思是 检测器
19:50:37	  BeyondRiver : 检测器和分类器的区别
19:50:40	  96459666 : 。。。
19:50:51	  96459666 : 直接略过了
19:51:18	  32245254 : 唉
19:51:50	  96459666 : 老师 你能看到我们聊天窗口的提问吗
19:52:18	  BUG_MAN : 这里只有2个，投票感觉意义大不
19:53:48	  Eason060 : 有没有推荐的resource后面可以发布下？这样讲感觉有点浅
19:54:02	  BeyondRiver : 我还是想问问event检测器 和 分类器的区别
19:54:03	  sc_lj : 机器翻译，特别是小语种的机器翻译有什么好的文献推荐吗？
19:54:11	  Leonzz : colearning什么算parallel呀
19:54:47	  hkc000 : 情感分类里边：2个检测器输出是什么
19:55:09	  hkc000 : 不是
19:55:12	  hkc000 : 下一把
19:55:17	  hkc000 : 下一个
19:55:25	  hkc000 : 上上个
19:55:34	  billyzhang24kobe : 对抗网络那个吧
19:55:36	  96459666 : 假新闻那个：像素这块  文字和图片信息分别是GRU和CNN提取的吗
19:55:37	  35494826 : 输入单张图片，过多个gru，是基于什么考虑呢
19:55:37	  hkc000 : 哎呀找不到了
19:55:39	  SimonChen : 假新闻识别的
19:56:17	  Leonzz : biGRU output是什么
19:56:36	  zhengjiang : bert能用到多模态吗
19:56:41	  96459666 : 它这输入进去的不是图片吗
19:57:19	  hkc000 : 假新闻2018那篇
19:57:34	  96459666 : GRU不是提取上下文信息吗？  图片里有文字？还是？
19:57:36	  32245254 : 那不就是只有图像一个模态吗
19:57:43	  hbfuyue : 模型评价假新闻的标准是什么？
19:57:43	  BeyondRiver : 老师，在最后一个假新闻模型里 event检测器具体是什么，和分类器有什么区别
19:58:48	  32245254 : 模型评价假新闻的标准是什么？是图文不符？？
19:58:53	  BeyondRiver : 哦哦
19:58:55	  韩晓东 : 这是两个任务
19:58:56	  SimonChen : 检测器和下面的那块同时进行吗
19:59:04	  Eason060 :  event检测器是干嘛的，能给个例子吗
19:59:04	  SimonChen : 紫色和绿色
19:59:15	  32245254 : 。。。
19:59:16	  Eason060 : 判断这是什么事件？
19:59:26	  hkc000 : 绿色那块输出的是？？事件分类吗
19:59:47	  96459666 : 。。。。
19:59:48	  32245254 : 。。。
19:59:48	  65858525 : 对呀假新闻是什么呀
19:59:52	  32245254 : 什么鬼
19:59:53	  96459666 : 这
19:59:55	  hyalin : ......
20:00:01	  96459666 : 这老师醉了
20:01:03	  96459666 : 这节课等于就是丢一个论文清单
20:01:26	  96459666 : 那不如直接上paper课
20:02:25	  65858525 : 兄弟们问一下，到底假新闻的定义是什么呀....
20:02:30	  32245254 : 该反映的直接联系班主任或者文哲老师。。咱这吐槽也没用
20:02:41	  于怼怼 : 我觉得假新闻就是图文不符
20:02:43	  ls : 假新闻应该有标签吧
20:02:49	  32245254 : 问了好几次啊 这老师就是不说啊
20:02:54	  SimonChen : 假新闻应该就是虚假的新闻吧，并没有发生的事情？
20:02:59	  ls : 人工给标签的吧
20:03:10	  65858525 : 到底是图文不符还是虚假新闻呀....
20:03:22	  于怼怼 : 我的理解是图文不符
20:03:31	  32245254 : 应该是图文不符
20:03:42	  zhengjiang : 我看智源去年也有假新闻识别比赛
20:06:20	  滑车 : The multi-modal feature extractor is responsible for extracting the textual and visual features from posts. It cooperates with the fake news detector to learn the discriminable representation for the detection of fake news. The role of event discriminator is to remove the event-specific features and keep shared features among events.
20:06:25	  滑车 : 论文摘要原文
20:23:18	  滑车 : 1
20:23:18	  愚蠢的人类 : 1
20:23:20	  小柒 : 1
20:23:21	  韩晓东 : 1
20:23:21	  kexin : 1
20:23:27	  33216922 : 1
20:23:27	  46149090 : 1
20:23:39	  71730733 : 1
20:23:39	  星空str : 1
20:23:40	  于怼怼 : 1111
20:23:59	  张同学 : @
20:24:01	  张同学 : 2
20:24:02	  SimonChen : 2
20:24:02	  kexin : 2
20:24:05	  71730733 : 2
20:24:08	  于怼怼 : 只用过tensorflow
20:24:12	  zhengjiang : 2
20:24:16	  hbfuyue : 只用过tensorflow
20:29:18	  33216922 : 1
20:29:18	  SimonChen : 1
20:29:18	  kin : 1
20:29:18	  72352735 : 2
20:29:18	  滑车 : 2
20:29:19	  46149090 : 2
20:29:19	  有朝一日 : 1
20:29:19	  Lee : 1
20:29:19	  hbfuyue : 2
20:29:20	  12655582 : 2
20:29:21	  可立 : 2
20:29:21	  ls : 1
20:29:22	  rabbitskip : 2
20:29:22	  于怼怼 : 22222222
20:29:24	  Will : 2
20:29:24	  71730733 : 1
20:29:24	  ffffff : 1
20:29:26	  20819655 : 1
20:29:26	  hy : 1
20:29:29	  liuf : 2
20:29:41	  95960842 : 都没用过扣0么
20:29:43	  95960842 : 000
20:29:44	  可立 : 1&2
20:30:17	  yk : 0
20:30:27	  82680142 : 0
20:30:30	  愚蠢的人类 : 0
20:35:17	  hbfuyue : 字太小
20:35:33	  hbfuyue : ok
20:37:59	  可立 : 啊？不会自动转换的吗
20:39:12	  可立 : 哦会自动转换
20:47:19	  96459666 : 这梯度是干嘛的
20:48:36	  shisihuan24 : grand_fn什么意思
20:48:44	  96459666 : 就是为啥要设置那个参数
20:49:02	  96459666 : torch的tensor要求必须给的吗
20:49:05	  BeyondRiver : fn就是function
20:49:12	  BeyondRiver : 就是你上次什么操作
20:49:17	  96459666 : 哦哦
20:49:35	  SimonChen : 老师，我想问一下，这个reqires我们是对input添加还是对初始化的权重添加？
20:51:04	  BeyondRiver : 看doc吧
20:51:16	  BeyondRiver : 甚至可以看个有例子的
20:51:38	  BeyondRiver : 搜init?
20:52:10	  96459666 : 老师 还是继续往下讲吧。
20:52:17	  hkc000 : 默认false吧
20:52:30	  滑车 : W X , W True, X false
20:52:38	  shisihuan24 : 字变小啦
20:57:58	  hbfuyue : 这个字更小
20:58:10	  BeyondRiver : 别在手机上看code
20:58:25	  hbfuyue : 在电脑上看到
20:58:34	  BeyondRiver : 可以放大
20:58:43	  BeyondRiver : zhumu ratio调整
20:58:49	  hbfuyue : 好
21:03:56	  BeyondRiver : weight default是什么来着
21:04:05	  BeyondRiver : 我能在看一眼源码吗
21:04:08	  BeyondRiver : nvm
21:04:49	  Zhuangboxu : 注释写错了
21:05:07	  Zhuangboxu : Tanh 和Sigmoid
21:09:13	  Zhuangboxu : 28-3+1
21:09:33	  liuf : 没有padding
21:09:46	  赵诚宇 : 如果卷积核不是方阵该怎么确定kernel size
21:09:46	  kexin : 核函数把size压缩了
21:11:17	  胡云聪 : 压缩深度
21:11:27	  liuf : 改变channal
21:11:39	  kexin : channal 变大了
21:12:08	  12655582 : 缩放
21:13:10	  hbfuyue : 什么是input channal和output channal?
21:13:30	  61116528 : 字体能大一点吗
21:13:33	  BeyondRiver : 先run一个函数再下一个
21:13:46	  BeyondRiver : 字体看不清的最上方可以缩放
21:14:28	  SimonChen : 老师，我还是不太懂那个1*1的卷积，能再说一下吗？
21:14:30	  20819655 : 老师要考虑一下没有基础不太懂的同学，别讲那么快，刚才的1*1 kernel_size我还不懂。。。
21:15:09	  Will : 各位，真的要预习
21:15:24	  Eason060 : 这还快 真的够细了
21:15:27	  liuf : 这块主要是图像相关的
21:15:38	  BeyondRiver : 其实这里是卷积神经网络的知识
21:17:26	  96459666 : 吴恩达深度学习讲CNN基础还是挺不错的
21:17:39	  于怼怼 : 深刻感觉pytorch比tensorflow更通俗易懂呀
21:18:42	  Eason060 : 是的 代码更python
21:18:54	  滑车 : tensorflow 用 keras API 的话，也差不多
21:23:31	  liuf : slim也类似 不过tf不好调试 要用session
21:23:55	  hkc000 : 2.0不用
21:24:15	  96459666 : 这-1和-3是啥
21:24:17	  96459666 : 概率吗？
21:24:46	  Eason060 : 不是概率 是posterior，softmax之后还是概率
21:24:56	  Eason060 : *才是
21:25:07	  Lee : 假设的输出预测，做完softmax之后是概率
21:25:14	  96459666 : soga
21:26:57	  96459666 : distill.pub/2017/momentum
21:27:25	  Lee : ill-conditioned Problem
21:28:02	  kexin : 虽然最终结果更好，但Momentum 会让收敛性速度变慢吗？
21:28:53	  胡云聪 : 可能会over shoot吧
21:34:08	  滑车 : bucket ？
21:34:11	  BeyondRiver : 妙啊
21:34:24	  BeyondRiver : neat
21:35:39	  ChenS : Dataset + data loader这样做的好处是啥，省内存？
21:36:38	  ChenS : 这个在spark上，每个batch 送到RDD上，parallel 跑吗
21:36:46	  ChenS : 可以不
21:37:21	  BeyondRiver : 我觉得从programming的角度说
21:37:26	  BeyondRiver : 还是用loader好
21:37:28	  可立 : 感觉还是tf.data.Dataset功能多一些
21:37:37	  可立 : 可以cache吗
21:38:05	  hkc000 : 是不是有个叫torchnlp的工具
21:38:15	  hkc000 : 和torchvision一样
21:38:40	  20819655 : torchtext吧
21:38:59	  57325467 : mnist, not minst
21:40:33	  Eason060 : ？
21:40:35	  61116528 : ？
21:40:36	  27473321 : ?
21:40:38	  hkc000 : ？
21:40:38	  lyj : 卡了？
21:40:39	  BeyondRiver : ??
21:40:39	  愚蠢的人类 : 卡了？
21:40:40	  20819655 : ?
21:41:38	  shisihuan24 : super() 是啥?
21:42:00	  滑车 : 调用父类方法
21:42:02	  BeyondRiver : 继承
21:42:27	  Lee : 你可以理解为调用父类方法
21:43:57	  shisihuan24 : 谢谢
21:45:08	  Eason060 : pytorch是真的舒服
21:45:42	  BeyondRiver : 是真的 programming style很OOD
21:46:12	  于怼怼 : 自己写代码时，都需要调用这个optimizer.zero_grad()么？不调用会有啥后果?
21:46:12	  滑车 : 等要部署模型的时候，就知道 tensorflow 的好了
21:46:32	  Lee : 梯度会累计
21:46:38	  BeyondRiver : grad要清零的 老师前面讲过
21:46:48	  hkc000 : OOD是？？
21:47:15	  46149090 : 能查看每个参数在不同echo的梯度吗、
21:47:16	  于怼怼 : 懂了，谢谢
21:47:21	  hkc000 : 为什么要设置累计梯度的机制？？
21:47:32	  BeyondRiver : 可以查看梯度
21:47:33	  Eason060 : OOD就是面对对象
21:47:48	  BeyondRiver : %
21:48:22	  hkc000 : 懂了。
21:50:24	  于怼怼 : 老师，pytorch模型部署时，用啥部署？
21:50:49	  96459666 : 老师
21:50:53	  96459666 : pytorch部署
21:50:58	  96459666 : Pb模型怎么转换
21:51:04	  96459666 : 还是直接是固化的模型额
21:51:56	  滑车 : https://pytorch.org/blog/model-serving-in-pyorch/
21:52:01	  96459666 : soga
21:52:17	  萝卜 : tensorflow的模型可以转换成pytorch可以load的嘛
21:53:42	  72352735 : 老师，刚才pycharm里边的代码可以分享吗
21:54:01	  lyj : jupyter的代码也分享一下
21:54:05	  72352735 : 谢谢老师！
21:54:07	  96459666 : 老师好棒
21:54:07	  BeyondRiver : pycharm里的那是源码
21:54:27	  96459666 : 老师顺便搜刮一下好的关于用bert的pyrotch代码
21:54:29	  96459666 : 分享下下
21:54:39	  ChenS : 老师，想问下，可以这每个batch 送到spark RDD上，parallel 跑吗
21:54:56	  贪心学院小龙 : transformers
21:55:07	  20819655 : 目测老师研究知识图谱，有机会可以分享一下吗？
21:55:11	  72352735 : https://github.com/huggingface
21:55:36	  72352735 : https://github.com/huggingface/transformers
21:55:46	  BeyondRiver : https://github.com/huggingface/transformers
21:56:15	  20819655 : 好的，谢谢老师！
21:56:18	  lyj : 谢谢老师
21:56:22	  limingkailmk : 谢谢老师
21:56:26	  72352735 : 谢谢老师！
21:56:28	  愚蠢的人类 : 感谢老师
21:56:29	  ls : 谢谢老师
21:56:32	  Lee : 谢谢老师
21:56:32	  ChenS : 谢谢老师
21:56:33	  billyzhang24kobe : 谢谢老师
21:56:35	  SimonChen : thx
21:56:36	  韩晓东 : 谢谢老师
21:56:37	  ww5365 : 谢谢
21:56:37	  ttsama : 谢谢老师
21:56:37	  liuf : 老师讲的很好
21:56:40	  Loop : 老师拜拜
21:56:42	  胡云聪 : 谢谢老师
21:56:47	  liuf : 谢谢老师
21:56:51	  61116528 : 老师讲的很好
21:56:51	  ChenS : 以后可以长一点啊时间
21:56:55	  hkc000 : 是啊
21:56:58	  20819655 : 可以长一点~
21:56:59	  BeyondRiver : 没听够
21:57:06	  hkc000 : 一小时太短了
21:57:07	  27473321 : 完全没听够
21:57:10	  ww5365 : 我觉得时间，太少了，课程可以长点。
21:57:16	  liuf : 没有无聊 多讲点
21:57:20	  rabbitskip : 老师讲得很好，还可以长一些
21:57:21	  BeyondRiver : 没关系 我们会了再听可以学习
21:57:26	  BeyondRiver : 总有新发现
21:57:28	  于怼怼 : 都是王老师这个水平，那这课值5万
21:57:31	  hkc000 : 讲到代码大家都很积极
21:57:32	  lyj : hh
21:57:36	  Loop : hhh
21:57:37	  lyj : 又来了你
21:57:37	  limingkailmk : 哈哈
21:57:46	  17080518 : 课值5万
21:57:51	  ww5365 : 时间长点
21:58:00	  89939482 : 建议于对对自己额外补缴3万
21:58:00	  ttsama : 彩虹屁
21:58:04	  hkc000 : 哈哈
21:58:34	  BeyondRiver : 读doc 可以solve 90%
21:58:44	  萝卜 : 谢谢老师
21:58:45	  20819655 : 拜拜👋
21:58:46	  lyj : 好的，老师再见
21:58:47	  于怼怼 : 谢谢老师
21:59:02	  20819655 : 大家再见~
